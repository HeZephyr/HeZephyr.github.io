<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>【论文阅读笔记】Attention Is All You Need | ZephyrHe</title><meta name=author content="HeZephyr">
<meta name=author-link content="https://github.com/HeZephyr"><meta name=description content="1 引言
  
    
  
循环神经网络(RNN)，特别是长短期记忆和门控循环神经网络（编码器-解码器架构），已成为序列建模和转换问题（如语言建模和机器翻译）的先进方法，众多研究在不断推动其发展。"><meta name=keywords content='Transformer,模型,人工智能'><meta itemprop=name content="【论文阅读笔记】Attention Is All You Need"><meta itemprop=description content="1 引言 循环神经网络(RNN)，特别是长短期记忆和门控循环神经网络（编码器-解码器架构），已成为序列建模和转换问题（如语言建模和机器翻译）的先进方法，众多研究在不断推动其发展。"><meta itemprop=datePublished content="2024-07-07T00:00:00+00:00"><meta itemprop=dateModified content="2024-10-15T13:58:47+00:00"><meta itemprop=wordCount content="4277"><meta itemprop=image content="https://hezephyr.github.io/images/apple-devices-preview.webp"><meta itemprop=keywords content="Transformer,模型,人工智能"><meta property="og:url" content="https://hezephyr.github.io/posts/04.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0transformer/"><meta property="og:site_name" content="ZephyrHe"><meta property="og:title" content="【论文阅读笔记】Attention Is All You Need"><meta property="og:description" content="1 引言 循环神经网络(RNN)，特别是长短期记忆和门控循环神经网络（编码器-解码器架构），已成为序列建模和转换问题（如语言建模和机器翻译）的先进方法，众多研究在不断推动其发展。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-07-07T00:00:00+00:00"><meta property="article:modified_time" content="2024-10-15T13:58:47+00:00"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="模型"><meta property="article:tag" content="人工智能"><meta property="og:image" content="https://hezephyr.github.io/images/apple-devices-preview.webp"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://hezephyr.github.io/images/apple-devices-preview.webp"><meta name=twitter:title content="【论文阅读笔记】Attention Is All You Need"><meta name=twitter:description content="1 引言 循环神经网络(RNN)，特别是长短期记忆和门控循环神经网络（编码器-解码器架构），已成为序列建模和转换问题（如语言建模和机器翻译）的先进方法，众多研究在不断推动其发展。"><meta name=application-name content="Zephyr's Blog"><meta name=apple-mobile-web-app-title content="Zephyr's Blog"><meta name=theme-color data-light=#ffffff data-dark=#252627 content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://hezephyr.github.io/posts/04.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0transformer/><link rel=prev href=https://hezephyr.github.io/posts/07.shell%E5%AE%9E%E6%88%98/><link rel=next href=https://hezephyr.github.io/posts/05.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0deepcad/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://unpkg.com/@fortawesome/fontawesome-free@6.4.2/css/all.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://unpkg.com/@fortawesome/fontawesome-free@6.4.2/css/all.min.css></noscript><link rel=preload href=https://unpkg.com/animate.css@4.1.1/animate.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://unpkg.com/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"【论文阅读笔记】Attention Is All You Need","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/hezephyr.github.io\/posts\/04.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0transformer\/"},"image":[{"@type":"ImageObject","url":"https:\/\/hezephyr.github.io\/images\/apple-devices-preview.webp","width":2880,"height":1508}],"genre":"posts","keywords":"Transformer, 模型, 人工智能","wordcount":4277,"url":"https:\/\/hezephyr.github.io\/posts\/04.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0transformer\/","datePublished":"2024-07-07T00:00:00+00:00","dateModified":"2024-10-15T13:58:47+00:00","license":"本站内容采用 CC BY-NC-SA 4.0 国际许可协议。","publisher":{"@type":"Organization","name":"Lruihao","logo":"https:\/\/hezephyr.github.io\/images\/avatar.jpg"},"author":{"@type":"Person","name":"HeZephyr"},"description":""}</script></head><body data-header-desktop=sticky data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=/ title=ZephyrHe><img loading=lazy src=/images/logo.png alt=ZephyrHe data-title=ZephyrHe width=26 height=26 class=logo style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>HeZephyr</span></a><span class=header-subtitle>贺志飞的博客</span></div><nav><ul class=menu><li class="menu-item has-children"><a class=menu-link href=/archives/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 归档</a><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i><ul class=sub-menu><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-folder-tree fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/collections/><i class="fa-solid fa-layer-group fa-fw fa-sm" aria-hidden=true></i> 合集</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li></ul></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class=menu-item><a class=menu-link href=/about/><i class="fa-solid fa-signature fa-fw fa-sm" aria-hidden=true></i> 关于</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=ZephyrHe><img loading=lazy src=/images/logo.png alt=ZephyrHe data-title=ZephyrHe width=26 height=26 class=logo style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>HeZephyr</span></a><span class=header-subtitle>贺志飞的博客</span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><span class=nested-item><a class=menu-link href=/archives/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 归档</a>
<i class="dropdown-icon fa-solid fa-chevron-right" aria-hidden=true></i></span><ul class=sub-menu><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-folder-tree fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/collections/><i class="fa-solid fa-layer-group fa-fw fa-sm" aria-hidden=true></i> 合集</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li></ul></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class=menu-item><a class=menu-link href=/about/><i class="fa-solid fa-signature fa-fw fa-sm" aria-hidden=true></i> 关于</a></li><li class="menu-item text-center"><a class=menu-link href=https://github.com/HeZephyr/HeZephyr.github.io title=GitHub rel="noopener noreferrer" target=_blank><i class='fa-brands fa-github fa-fw' aria-hidden=true></i></a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><nav aria-label=breadcrumb class="breadcrumb-container sticky"><ol class=breadcrumb><li class=breadcrumb-item><a href=/posts/ title=Posts>文章</a></li><li class="breadcrumb-item active" aria-current=page>【论文阅读笔记】Attention Is All You Need</li></ol></nav><main class=container><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label=合集><div class="details collection-details open"><div class="details-summary collection-summary"><i class="fa-solid fa-layer-group fa-fw" aria-hidden=true></i>
<span class=collection-name data-collections=合集>论文阅读笔记</span>
<span class=collection-count>11</span><i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class="details-content collection-content"><nav><ul class=collection-list><li class=collection-item><a href=/posts/01.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0mapreduce/ title="【论文阅读笔记】MapReduce: Simplified Data Processing on Large Clusters">【论文阅读笔记】MapReduce: Simplified Data Processing on Large Clusters</a></li><li class=collection-item><a href=/posts/02.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0gfs/ title="【论文阅读笔记】The Google File System">【论文阅读笔记】The Google File System</a></li><li class=collection-item><span class=active title="【论文阅读笔记】Attention Is All You Need">【论文阅读笔记】Attention Is All You Need</span></li><li class=collection-item><a href=/posts/05.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0deepcad/ title="【论文阅读笔记】DeepCAD: A Deep Generative Network for Computer-Aided Design Models">【论文阅读笔记】DeepCAD: A Deep Generative Network for Computer-Aided Design Models</a></li><li class=collection-item><a href=/posts/06.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0skexgen/ title="【论文阅读笔记】 SkexGen: Autoregressive Generation of CAD Construction Sequences With Disentangled Codebooks">【论文阅读笔记】 SkexGen: Autoregressive Generation of CAD Construction Sequences With Disentangled Codebooks</a></li><li class=collection-item><a href=/posts/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0hierarchical-neural-coding-for-controllable-cad-model-generation/ title="【论文阅读笔记】Hierarchical Neural Coding for Controllable CAD Model Generation">【论文阅读笔记】Hierarchical Neural Coding for Controllable CAD Model Generation</a></li><li class=collection-item><a href=/posts/03.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0in-search-of-an-understandable-consensus-algorithm-extended-version/ title="【论文阅读笔记】In Search of an Understandable Consensus Algorithm (Extended Version)">【论文阅读笔记】In Search of an Understandable Consensus Algorithm (Extended Version)</a></li><li class=collection-item><a href=/posts/08.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0zookeeper-wait-free-coordination-for-internet-scale-systems/ title="【论文阅读笔记】ZooKeeper: Wait-Free Coordination for Internet-Scale Systems">【论文阅读笔记】ZooKeeper: Wait-Free Coordination for Internet-Scale Systems</a></li><li class=collection-item><a href=/posts/09.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0grove/ title="【论文阅读笔记】Grove: A Separation-Logic Library for Verifying Distributed Systems (Extended Version)">【论文阅读笔记】Grove: A Separation-Logic Library for Verifying Distributed Systems (Extended Version)</a></li><li class=collection-item><a href=/posts/11.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0bigtable/ title="【论文阅读笔记】Bigtable: A Distributed Storage System for Structured Data">【论文阅读笔记】Bigtable: A Distributed Storage System for Structured Data</a></li><li class=collection-item><a href=/posts/10.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0chubby/ title="【论文阅读笔记】The Chubby Lock Service for Loosely-Coupled Distributed Systems">【论文阅读笔记】The Chubby Lock Service for Loosely-Coupled Distributed Systems</a></li></ul><div class=collection-nav-simple><a href=/posts/02.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0gfs/ class=collection-nav-item rel=prev title="【论文阅读笔记】The Google File System"><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i></a><span class=text-secondary>3/11</span><a href=/posts/05.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0deepcad/ class=collection-nav-item rel=next title="【论文阅读笔记】DeepCAD: A Deep Generative Network for Computer-Aided Design Models"><i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></nav></div></div></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>【论文阅读笔记】Attention Is All You Need</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/HeZephyr title=作者 target=_blank rel="external nofollow noopener noreferrer author" class=author><img loading=lazy src="https://cn.gravatar.com/avatar/65f6ab71abcb9d540eb32a2704884927?s=32&amp;d=mp" alt=HeZephyr data-title=HeZephyr width=20 height=20 class=avatar style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'>&nbsp;HeZephyr</a></span><span class=post-included-in>&nbsp;收录于 <a href=/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ class=post-category title="分类 - 深度学习"><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> 深度学习</a>&ensp;<a href=/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/ class=post-category title="分类 - 论文阅读"><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> 论文阅读</a> 和 <a href=/collections/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/ class=post-collection title="合集 - 论文阅读笔记"><i class="fa-solid fa-layer-group fa-fw" aria-hidden=true></i> 论文阅读笔记</a></span></div><div class=post-meta-line><span title="发布于 2024-07-07 00:00:00"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden=true></i><time datetime=2024-07-07>2024-07-07</time></span>&nbsp;<span title="更新于 2024-10-15 13:58:47"><i class="fa-regular fa-calendar-check fa-fw me-1" aria-hidden=true></i><time datetime=2024-10-15>2024-10-15</time></span>&nbsp;<span title="4277 字"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 4300 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 9 分钟</span>&nbsp;<span id=busuanzi_container_page_pv class="busuanzi_visitors comment-visitors" data-flag-title="【论文阅读笔记】Attention Is All You Need">
<i class="fa-regular fa-eye fa-fw me-1" aria-hidden=true></i><span id=busuanzi_value_page_pv>-</span>&nbsp;次阅读
</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ol><li><a href=#引言>引言</a></li><li><a href=#背景>背景</a></li><li><a href=#模型架构>模型架构</a><ol><li><a href=#编码器堆叠和解码器堆叠>编码器堆叠和解码器堆叠</a></li><li><a href=#注意力机制>注意力机制</a><ol><li><a href=#缩放的点积注意力机制>缩放的点积注意力机制</a></li><li><a href=#多头注意力机制>多头注意力机制</a></li><li><a href=#heading></a></li><li><a href=#transformer中注意力机制的应用>Transformer中注意力机制的应用</a></li></ol></li><li><a href=#逐位置全连接前馈神经网络>逐位置全连接前馈神经网络</a></li><li><a href=#transformer中的逐位置全连接前馈神经网络>Transformer中的逐位置全连接前馈神经网络</a></li><li><a href=#嵌入和softmax>嵌入和Softmax</a></li><li><a href=#位置编码>位置编码</a></li></ol></li><li><a href=#为什么使用自注意力机制>为什么使用自注意力机制</a></li><li><a href=#训练>训练</a><ol><li><a href=#优化器>优化器</a></li><li><a href=#正则化>正则化</a></li></ol></li></ol></nav></div></div><div class=content id=content data-end-flag="「本文完，更多内容汇总在我的 GitHub与CSDN，持续更新，求关注、求 star、求订阅！」"><h2 id=引言 class=heading-element><span>1 引言</span>
<a href=#%e5%bc%95%e8%a8%80 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>循环神经网络(RNN)，特别是长短期记忆和门控循环神经网络（<strong>编码器-解码器</strong>架构），已成为序列建模和转换问题（如语言建模和机器翻译）的先进方法，众多研究在不断推动其发展。</p><p><a class=lightgallery href="https://raw.githubusercontent.com/unique-pure/NewPicGoLibrary/main/img/RNN?size=large" data-thumbnail="https://raw.githubusercontent.com/unique-pure/NewPicGoLibrary/main/img/RNN?size=small" data-sub-html="<h2>RNN做机器翻译</h2>"><img loading=lazy src=https://raw.githubusercontent.com/unique-pure/NewPicGoLibrary/main/img/RNN alt=RNN做机器翻译 srcset="https://raw.githubusercontent.com/unique-pure/NewPicGoLibrary/main/img/RNN?size=small, https://raw.githubusercontent.com/unique-pure/NewPicGoLibrary/main/img/RNN?size=medium 1.5x, https://raw.githubusercontent.com/unique-pure/NewPicGoLibrary/main/img/RNN?size=large 2x" data-title=RNN做机器翻译 class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>但RNN通常<font color=red>沿输入和输出序列的符号位置</font>进行计算，其固有的顺序性导致训练示例内难以并行化，在序列长度较长时，由于内存限制跨示例的批处理，这一问题更加突出。尽管近期通过一些技术改进了计算效率，<font color=red>但顺序计算的基本限制仍未改变</font>。</p><p>且RNN使用共享权值矩阵，在面临较长序列时，会有梯度消失的问题(也可以说是后面词的梯度会覆盖前面的梯度)。即使后序的LSTM和GRU对这一部分做了改进，但也无法完全解决该问题。</p><p><font color=red>注意力机制</font>已成为各种序列建模和转换模型的重要组成部分，能在不考虑输入或输出序列距离的情况下对依赖关系进行建模，但在大多数情况下与循环网络结合使用。</p><p>作者提出了 Transformer 模型，该模型摒弃了循环单元和卷积单元，完全依赖注意力机制来建立输入和输出之间的全局依赖关系，允许更多的并行化。</p><h2 id=背景 class=heading-element><span>2 背景</span>
<a href=#%e8%83%8c%e6%99%af class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><font color=red>减少序列计算量和加速计算</font>是序列处理模型中的基本思想。ByteNet和ConvS2S通过使用卷积神经网络并行计算，计算量和序列中位置的距离相关。ConvS2S是线性关系，而ByteNet是对数关系，使得长距离关系学习困难。Transformer将这个过程减少到常数规模，尽管降低了有效分辨率，但<strong>多头注意力机制</strong>弥补了这一点。</p><p>自注意力机制（内部注意力机制）为<font color=red>序列的不同位置分配权重，并学习表示向量</font>，已在阅读理解、文本摘要等任务中表现出色。</p><p>端到端记忆网络通常基于循环注意力机制，已用于简单语言翻译等任务。而Transformer 是<font color=red>第一个完全依赖自注意力</font>来计算其输入和输出表示的转换模型。</p><h2 id=模型架构 class=heading-element><span>3 模型架构</span>
<a href=#%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>Transformer中沿用了非常经典的编码器-解码器架构，编码器将输入的序列$(x_1,\dots,x_n)$转化成一个表示向量$\boldsymbol{z}=(z_1,\dots,z_n)$，而编码器依据向量$\boldsymbol{z}$逐步生成输出序列$(y_1,\dots,y_m)$，并且模型在每个步骤中都是<strong>自回归</strong>的，会将先前生成的符号作为生成下一个的额外输入，例如这一步要生成$y_t$，要将$(y_1,\dots,y_{t-1})$都拿到也作为输入。</p><p>同时Transformer模型在编码器和解码器中都使用堆叠自注意力机制和逐点全连接层，如下图的左半部分和右半部分所示。</p><img src=https://raw.githubusercontent.com/unique-pure/NewPicGoLibrary/main/img/Transformer_Architecture alt=image-20240707194838925 style=zoom:33%><h3 id=编码器堆叠和解码器堆叠 class=heading-element><span>3.1 编码器堆叠和解码器堆叠</span>
<a href=#%e7%bc%96%e7%a0%81%e5%99%a8%e5%a0%86%e5%8f%a0%e5%92%8c%e8%a7%a3%e7%a0%81%e5%99%a8%e5%a0%86%e5%8f%a0 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>编码器由 $6$ 个相同的层堆叠而成。每个层包含两个子层：<font color=red>多头自注意力机制和逐位置全连接前馈神经网络</font>。每个子层都使用了残差连接，然后进行层规范化（<strong>防止模型过拟合</strong>）。假设每一层的输入是$x$，那么每一层的输出结果可以表示为：
$$
\text{LayerNorm}(x+\text{Sublayer(x)})
$$
其中</p><ul><li>$\text{SubLayer}$是当前子层本身实现的运算函数，比如注意力运算和全连接运算；</li><li>模型中的所有子层以及嵌入层的输出维度均为 $d_{\text{model}} = 512$（便于残差连接）。</li></ul><p>解码器同样由 6 个层组成。其结构与编码器类似，但多了一个<strong>对编码器输出进行关注的多头注意力子层</strong>。并且在<strong>自注意力子层中进行了修改</strong>，以防止信息左向流动。这种掩码机制，结合输出嵌入向量向后偏移一个位置，确保位置 $i$ 的预测仅依赖于位置小于 $i$ 的已知输出。</p><h3 id=注意力机制 class=heading-element><span>3.2 注意力机制</span>
<a href=#%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>注意力机制就是对一系列的query和一系列的key-value对，我们需要确定对于每个query而言不同 value 的重要程度，而这个权重是根据 query 和key 的相关度计算得到的。</p><h4 id=缩放的点积注意力机制 class=heading-element><span>3.2.1 缩放的点积注意力机制</span>
<a href=#%e7%bc%a9%e6%94%be%e7%9a%84%e7%82%b9%e7%a7%af%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><p>Transformer模型中使用的是缩放的点积注意力机制。在这种机制中，注意力计算涉及query和key的维度为 $d_k$ )，以及value的维度为 $d_v$ 。通过引入一个<strong>缩放因子</strong>$\sqrt{d_k}$，可以控制注意力分布的稳定性和计算效率。</p><img src=https://raw.githubusercontent.com/unique-pure/NewPicGoLibrary/main/img/Scaled_Dot_Product_Attention alt=image-20240707205349089 style=zoom:33%><p>我们用向量化的方式可以将这种注意力机制的计算过程表示成：
$$
\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{d_k}})V
$$
与传统的注意力机制相比，缩放的点积注意力计算速度更快。缩放参数用于调节注意力计算的规模，以确保对于不同大小的输入，注意力权重的计算结果都能保持在合理的范围内。特别是在$\text{softmax}$函数的应用中，由于指数函数的快速增长特性，<font color=red>缩放可以有效防止某些权重过大</font>，而其他权重接近零的情况，确保了计算结果的平稳性和有效性。</p><h4 id=多头注意力机制 class=heading-element><span>3.2.2 多头注意力机制</span>
<a href=#%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><h4 id=heading class=heading-element><span>3.2.3 </span><a href=#heading class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><p>Transformer采用了多头注意力机制，将query、key和value进行$h$次投影，然后对$h$个投影并行计算注意力，再将这些结果组合并线性投影生成最终的多头注意力输出。多头注意力使模型能够共同关注不同表示子空间和不同位置的信息。</p><img src=https://raw.githubusercontent.com/unique-pure/NewPicGoLibrary/main/img/Multi_Head_Attention alt=image-20240707214205509 style=zoom:50%><p>多头注意力机制的计算公式为：
$$
\operatorname{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O
$$
其中每个头的计算方式为：</p><p>$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$
参数的维度如下：</p><ul><li>$( Q, K, V)$ 的输入维度为 $( d_{model} )$（通常为$512$）</li><li>$W_i^Q \in \mathbb{R}^{d_{model} \times d_k}, W_i^K \in \mathbb{R}^{d_{model} \times d_k}, W_i^V \in \mathbb{R}^{d_{model} \times d_v}, W^O \in \mathbb{R}^{hd_v \times d_{model}}$</li></ul><p>作者在Transformer中设置 $h = 8$ 个头，每个头的维度为 $d_k = d_v = \frac{d_{model}}{h} = 64$。</p><h4 id=transformer中注意力机制的应用 class=heading-element><span>3.2.4 Transformer中注意力机制的应用</span>
<a href=#transformer%e4%b8%ad%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e7%9a%84%e5%ba%94%e7%94%a8 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><ul><li>在解码器的“编码器-解码器注意力层”中，query 来自上一个解码器层，而 key 和值 value 来自编码器的输出。这使得解码器中的每个位置都可以参与到输入序列所有位置的注意力计算中。</li></ul><ol><li><strong>编码器-解码器注意力层</strong>： 在这一层中，query 来自上一个解码器，而记忆 key 和 value 来自编码器的输出。这使得解码器中的每个位置都可以关注输入序列中的所有位置。</li><li><strong>编码器中的自注意力层</strong>： 编码器包含自注意力层。在这种自注意力机制中，<font color=red>query, key 和 value 都来自同一位置，即编码器前一层的输出</font>。这样，编码器中的每个位置都可以关注编码器前一层中的所有位置，从而捕捉输入序列中不同位置之间的全局依赖关系。</li><li><strong>解码器中的自注意力层</strong>： 解码器中的自注意力层的key，value和query也是同源的。但为了保持自回归属性，防止序列中前面的内容被后面的内容所影响，解码器在自注意力计算中加入了掩码机制。具体来说，通过在缩放的点积注意力中，将$\operatorname{softmax}$输入中对应非法连接的值设置为$-\infin$，来屏蔽这些连接。</li></ol><h3 id=逐位置全连接前馈神经网络 class=heading-element><span>3.3 逐位置全连接前馈神经网络</span>
<a href=#%e9%80%90%e4%bd%8d%e7%bd%ae%e5%85%a8%e8%bf%9e%e6%8e%a5%e5%89%8d%e9%a6%88%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><h3 id=transformer中的逐位置全连接前馈神经网络 class=heading-element><span>3.4 Transformer中的逐位置全连接前馈神经网络</span>
<a href=#transformer%e4%b8%ad%e7%9a%84%e9%80%90%e4%bd%8d%e7%bd%ae%e5%85%a8%e8%bf%9e%e6%8e%a5%e5%89%8d%e9%a6%88%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>在Transformer中，逐位置全连接前馈神经网络用于增强模型对序列中每个位置信息的处理能力，这种网络结构包含两个线性变换层和$\operatorname{ReLU}$激活函数，用于每个位置独立地进行相同的操作。其数学表示如下：</p><p>$$
\text{FFN}(x) = \max(0, x W_1 + b_1) W_2 + b_2
$$</p><p>虽然不同位置的线性变换相同，但各层使用的参数不同。其中，$x$ 是输入向量，维度为 $d_{\text{model}} = 512$ ，而内层的维度为$d_{ff}=2048$，$W_1$ 和 $W_2$ 是两个线性变换的权重矩阵，$b_1$ 和 $b_2$ 是相应的偏置向量。这个结构类似于<code>kernel size</code>为1的卷积操作。</p><h3 id=嵌入和softmax class=heading-element><span>3.5 嵌入和Softmax</span>
<a href=#%e5%b5%8c%e5%85%a5%e5%92%8csoftmax class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>和其他序列转换模型类似，Transformer 使用学习得到的嵌入将输入的token和输出的token转换为维度为 $ d_{\text{model}} $ 的向量。Transformer 还使用学习得到的线性变换和$\text{softmax}$函数将解码器的输出转换为预测的下一个token的概率。在Transformer中两个嵌入层和$\text{softmax}$之前的线性变换层之间共享相同的权重矩阵（<strong>减少模型的参数数量，降低过拟合的风险</strong>），同时在嵌入层中，我们将这些权重乘以 $\sqrt{d_{\text{model}}}$（<strong>缩放嵌入权重，防止梯度消失</strong>）。</p><h3 id=位置编码 class=heading-element><span>3.6 位置编码</span>
<a href=#%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>在Transformer模型中，由于没有循环和卷积单元，为了处理序列数据的位置信息，引入了位置编码。位置编码是为序列中的每个位置添加特定的向量表示，以便模型能够区分不同位置的token。在Transformer中的位置编码使用了sin函数和cos函数，这种方法不同于传统的学习得到的位置嵌入，而是采用固定的函数形式，即</p><p><a class=lightgallery href="https://raw.githubusercontent.com/HeZephyr/NewPicGoLibrary/main/img/test.image.latex-20240729215814533.png?size=large" data-thumbnail="https://raw.githubusercontent.com/HeZephyr/NewPicGoLibrary/main/img/test.image.latex-20240729215814533.png?size=small" data-sub-html="<h2>img</h2>"><img loading=lazy src=https://raw.githubusercontent.com/HeZephyr/NewPicGoLibrary/main/img/test.image.latex-20240729215814533.png alt=img srcset="https://raw.githubusercontent.com/HeZephyr/NewPicGoLibrary/main/img/test.image.latex-20240729215814533.png?size=small, https://raw.githubusercontent.com/HeZephyr/NewPicGoLibrary/main/img/test.image.latex-20240729215814533.png?size=medium 1.5x, https://raw.githubusercontent.com/HeZephyr/NewPicGoLibrary/main/img/test.image.latex-20240729215814533.png?size=large 2x" data-title=img style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>这里的$\text{pos}$表示位置而$i$表示维度，也就是对于位于$\text{pos}$位置的token的嵌入向量第$i$维加上这样一个值。</p><h2 id=为什么使用自注意力机制 class=heading-element><span>4 为什么使用自注意力机制</span>
<a href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e4%bd%bf%e7%94%a8%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>在处理序列数据时，长距离依赖关系的学习是一个关键挑战。论文中对比了使用自注意力、循环单元和卷积单元等不同模型结构时的计算量、时间复杂度和最大依赖路径长度。其中，最大依赖路径长度指的是任意两个输入输出位置之间信息传递的最长路径。</p><p><a class=lightgallery href="https://raw.githubusercontent.com/unique-pure/NewPicGoLibrary/main/img/Maximum_Path_Lengths?size=large" data-thumbnail="https://raw.githubusercontent.com/unique-pure/NewPicGoLibrary/main/img/Maximum_Path_Lengths?size=small" data-sub-html="<h2>image-20240707224337637</h2>"><img loading=lazy src=https://raw.githubusercontent.com/unique-pure/NewPicGoLibrary/main/img/Maximum_Path_Lengths alt=image-20240707224337637 srcset="https://raw.githubusercontent.com/unique-pure/NewPicGoLibrary/main/img/Maximum_Path_Lengths?size=small, https://raw.githubusercontent.com/unique-pure/NewPicGoLibrary/main/img/Maximum_Path_Lengths?size=medium 1.5x, https://raw.githubusercontent.com/unique-pure/NewPicGoLibrary/main/img/Maximum_Path_Lengths?size=large 2x" data-title=image-20240707224337637 class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>在传统的循环神经网络（RNN）中，信息是逐步传递的，因此全局视角下，任意两个位置之间的最长信息传递路径往往是以序列长度 $n$ 为量级的。这种逐步传递导致了RNN在捕捉长距离依赖时可能面临的挑战，尤其是在处理长序列时效果不佳。</p><p>相比之下，<font color=red>Transformer利用自注意力机制直接将每个位置与所有其他位置进行关联，避免了逐步传递的过程</font>，使得任意两个位置之间的信息传递路径变得极为直接和高效。这种直接的路径传递方式使得Transformer能够更有效地捕捉到长距离的依赖关系，而不受序列长度的限制。</p><p>因此，Transformer凭借其独特的注意力机制，实现了“Attention is all you need”的理念，强调了在序列建模中注意力机制的重要性和效果。它不仅仅是一种模型结构的创新，更是在解决长距离依赖问题上的一次重大突破。Transformer的成功不仅在于其高效的信息传递路径，还在于其能够在更大范围内捕捉和利用序列中的关联信息，从而提升了序列建模任务的性能和效果。</p><h2 id=训练 class=heading-element><span>5 训练</span>
<a href=#%e8%ae%ad%e7%bb%83 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>在论文中，训练Transformer模型涉及到几个关键的优化和正则化策略。</p><h3 id=优化器 class=heading-element><span>5.1 优化器</span>
<a href=#%e4%bc%98%e5%8c%96%e5%99%a8 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>模型的训练使用了Adam优化器，并采用了一种自适应的学习率。学习率 $\text{lr}$ 的计算方式如下所示：
<a class=lightgallery href="https://raw.githubusercontent.com/HeZephyr/NewPicGoLibrary/main/img/lr_learn_compute?size=large" data-thumbnail="https://raw.githubusercontent.com/HeZephyr/NewPicGoLibrary/main/img/lr_learn_compute?size=small" data-sub-html="<h2>img</h2>"><img loading=lazy src=https://raw.githubusercontent.com/HeZephyr/NewPicGoLibrary/main/img/lr_learn_compute alt=img srcset="https://raw.githubusercontent.com/HeZephyr/NewPicGoLibrary/main/img/lr_learn_compute?size=small, https://raw.githubusercontent.com/HeZephyr/NewPicGoLibrary/main/img/lr_learn_compute?size=medium 1.5x, https://raw.githubusercontent.com/HeZephyr/NewPicGoLibrary/main/img/lr_learn_compute?size=large 2x" data-title=img class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>这种自适应学习率的设计有助于在训练初期快速提升学习率，以加速模型收敛，而在训练后期逐渐降低学习率，以更细致地调整模型参数，提升训练的稳定性和效果。</p><h3 id=正则化 class=heading-element><span>5.2 正则化</span>
<a href=#%e6%ad%a3%e5%88%99%e5%8c%96 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>Transformer模型采用了多种正则化方法，以提升泛化能力和训练稳定性：</p><ol><li><p><strong>残差连接</strong>：在每个子层之间都使用了残差连接，这种连接方式有助于减少梯度消失问题，并简化了模型的训练和优化过程。</p></li><li><p><strong>Dropout</strong>：在输入嵌入向量和位置编码相加后的层中使用了Dropout，选择的Dropout概率为0.1。Dropout通过随机地将部分神经元的输出置为零，有助于防止模型过拟合，并增强泛化能力。</p></li><li><p><strong>标签平滑处理</strong>：这是一种用于改善模型训练和提高评价指标（如BLEU分数）的技术。标签平滑处理通过将真实标签替换为一个分布更平滑的目标分布，从而减少模型对训练数据中特定标签的过度自信，提升泛化能力和性能评估的一致性。</p></li></ol></div><hr class=awesome-hr><h2 id=see-also>相关内容</h2><ul><li><a href=/posts/03.%E6%B5%8B%E8%AF%95%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%BA%BF%E6%80%A7%E4%B8%80%E8%87%B4%E6%80%A7/ title="【MIT 6.5840(6.824)学习笔记】 测试分布式系统的线性一致性">【MIT 6.5840(6.824)学习笔记】 测试分布式系统的线性一致性</a></li><li><a href=/posts/39.andrew%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/ title=Andrew文件系统>Andrew文件系统</a></li><li><a href=/posts/37.%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/ title=分布式系统>分布式系统</a></li><li><a href=/posts/35.%E5%9F%BA%E4%BA%8E%E9%97%AA%E5%AD%98%E7%9A%84ssd/ title=基于闪存的SSD>基于闪存的SSD</a></li><li><a href=/posts/34.%E6%97%A5%E5%BF%97%E7%BB%93%E6%9E%84%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/ title=日志结构文件系统>日志结构文件系统</a></li></ul><div class=post-reward><div class=comment>Buy me a coffee~</div><input type=checkbox class=reward-input name=reward id=fi-reward hidden>
<label class=reward-button for=fi-reward><i class="fa-solid fa-qrcode fa-fw" aria-hidden=true></i>赞赏</label><div class=reward-ways data-mode=fixed><div><img loading=lazy src=/images/alipay.png alt="HeZephyr 支付宝" data-title="HeZephyr 支付宝" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'><span data-animation>支付宝</span></div><div><img loading=lazy src=/images/wechatpay.jpg alt="HeZephyr 微信" data-title="HeZephyr 微信" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'><span data-animation>微信</span></div></div></div><div class=collection-card><div class="collection-title text-secondary">收录于 <a href=/collections/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/><i class="fa-solid fa-layer-group fa-fw" aria-hidden=true></i> <span>合集・论文阅读笔记</span></span></a> 11</div><div class=collection-nav><a href=/posts/02.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0gfs/ class=collection-nav-item rel=prev title="【论文阅读笔记】The Google File System"><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i><span>【论文阅读笔记】The Google File System</span>
</a><a href=/posts/05.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0deepcad/ class=collection-nav-item rel=next title="【论文阅读笔记】DeepCAD: A Deep Generative Network for Computer-Aided Design Models"><span>【论文阅读笔记】DeepCAD: A Deep Generative Network for Computer-Aided Design Models</span><i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2024-10-15 13:58:47">更新于 2024-10-15&nbsp;</span></div><div class=post-info-license><span><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc-sa/4.0/ target=_blank>CC BY-NC-SA 4.0</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/posts/04.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0transformer/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span><span><a href="https://github.com/HeZephyr/HeZephyr.github.io/blob/docs/content/posts/_research/04.%e3%80%90%e8%ae%ba%e6%96%87%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0%e3%80%91Transformer.md?plain=1" title=查看源码 target=_blank rel="external nofollow noopener noreferrer" class=link-to-source>查看源码</a></span><span><a href=https://github.com/HeZephyr/HeZephyr.github.io/edit/docs/content/posts/_research/04.%e3%80%90%e8%ae%ba%e6%96%87%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0%e3%80%91Transformer.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span><span><a href="https://github.com/HeZephyr/HeZephyr.github.io/issues/new?title=[BUG]%20%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Attention+Is+All+You+Need&amp;body=%7cField%7cValue%7c%0A%7c-%7c-%7c%0A%7cTitle%7c%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Attention+Is+All+You+Need%7c%0A%7cURL%7chttps://hezephyr.github.io/posts/04.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0transformer/%7c%0A%7cFilename%7chttps://github.com/HeZephyr/HeZephyr.github.io/blob/docs/content/posts/_research/04.%e3%80%90%e8%ae%ba%e6%96%87%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0%e3%80%91Transformer.md?plain=1%7c" title=报告问题 target=_blank rel="external nofollow noopener noreferrer" class=link-to-report>报告问题</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 X" data-sharer=twitter data-url=https://hezephyr.github.io/posts/04.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0transformer/ data-title="【论文阅读笔记】Attention Is All You Need" data-hashtags=Transformer,模型,人工智能><i class="fa-brands fa-x-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://hezephyr.github.io/posts/04.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0transformer/ data-hashtag=Transformer><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Linkedin" data-sharer=linkedin data-url=https://hezephyr.github.io/posts/04.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0transformer/><i class="fa-brands fa-linkedin fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://hezephyr.github.io/posts/04.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0transformer/ data-title="【论文阅读笔记】Attention Is All You Need"><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 百度" data-sharer=baidu data-url=https://hezephyr.github.io/posts/04.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0transformer/ data-title="【论文阅读笔记】Attention Is All You Need"><i data-svg-src=https://unpkg.com/simple-icons@9.19.0/icons/baidu.svg aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/transformer/ class=post-tag title="标签 - Transformer">Transformer</a><a href=/tags/%E6%A8%A1%E5%9E%8B/ class=post-tag title="标签 - 模型">模型</a><a href=/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/ class=post-tag title="标签 - 人工智能">人工智能</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/05.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0deepcad/ class=post-nav-item rel=prev title="【论文阅读笔记】DeepCAD: A Deep Generative Network for Computer-Aided Design Models"><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>【论文阅读笔记】DeepCAD: A Deep Generative Network for Computer-Aided Design Models</a>
<a href=/posts/07.shell%E5%AE%9E%E6%88%98/ class=post-nav-item rel=next title="Shell 23道例题实战">Shell 23道例题实战<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments><div id=giscus class=comment><script src=https://giscus.app/client.js data-repo=HeZephyr/HeZephyr.github.io data-repo-id=R_kgDOMb2HgQ data-category=General data-category-id=DIC_kwDOMb2Hgc4ChPAb data-mapping=title data-strict=0 data-theme=preferred_color_scheme data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-lang=zh-CN data-loading=lazy crossorigin=anonymous async defer></script></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://giscus.app/ rel="external nofollow noopener noreferrer">giscus</a>.</noscript></div></article><aside class=toc id=toc-auto aria-label=目录><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class=toc-content id=toc-content-auto></div></aside></main><footer class=footer><div class=footer-container><div class="footer-line powered order-1">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.135.0"><img class=hugo-icon src=/images/hugo.min.svg alt="Hugo logo"> Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.3.9-RC"><img class=fixit-icon src=/images/fixit.min.svg alt="FixIt logo"> FixIt</a></div><div class="footer-line copyright order-2" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2020 - 2024</span><span class=author itemprop=copyrightHolder>
<a href=https://github.com/HeZephyr target=_blank rel="external nofollow noopener noreferrer">HeZephyr</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc-sa/4.0/ target=_blank>CC BY-NC-SA 4.0</a></span></div><div class="footer-line statistics"><span class=site-time title=网站运行中……><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden=true></i><span class="ms-1 d-none">博客已运行</span><span class="run-times ms-1">网站运行中……</span></span></div><div class="footer-line visitor order-first"><span id=busuanzi_container_site_uv title=总访客数><i class="fa-regular fa-user fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_uv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span><span id=busuanzi_container_site_pv class=footer-divider title=总访问量><i class="fa-regular fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_pv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div><div class="fixed-button view-comments d-none" role=button aria-label=查看评论><i class="fa-solid fa-comment fa-fw" aria-hidden=true></i></div></div><a href=https://github.com/HeZephyr title="View on GitHub" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true" width="56" height="56"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:#8581dd;--bg-progress-dark:#fff></div><noscript><div class=noscript-warning>该网站在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=https://unpkg.com/lightgallery@2.7.2/css/lightgallery-bundle.min.css><link rel=preload href=https://unpkg.com/katex@0.16.10/dist/katex.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://unpkg.com/katex@0.16.10/dist/katex.min.css></noscript><link rel=stylesheet href=https://unpkg.com/cookieconsent@3.1.1/build/cookieconsent.min.css><link rel=stylesheet href=https://unpkg.com/pace-js@1.2.4/themes/blue/pace-theme-minimal.css><script src=https://unpkg.com/autocomplete.js@0.38.1/dist/autocomplete.min.js defer></script><script src=https://unpkg.com/algoliasearch@4.20.0/dist/algoliasearch-lite.umd.js defer></script><script src=https://unpkg.com/instant.page@5.2.0/instantpage.js async defer type=module></script><script src=https://unpkg.com/lightgallery@2.7.2/lightgallery.min.js defer></script><script src=https://unpkg.com/lightgallery@2.7.2/plugins/thumbnail/lg-thumbnail.min.js defer></script><script src=https://unpkg.com/lightgallery@2.7.2/plugins/zoom/lg-zoom.min.js defer></script><script src=https://unpkg.com/sharer.js@0.5.1/sharer.min.js async defer></script><script src=https://unpkg.com/katex@0.16.10/dist/katex.min.js defer></script><script src=https://unpkg.com/katex@0.16.10/dist/contrib/auto-render.min.js defer></script><script src=https://unpkg.com/katex@0.16.10/dist/contrib/copy-tex.min.js defer></script><script src=https://unpkg.com/katex@0.16.10/dist/contrib/mhchem.min.js defer></script><script src=https://unpkg.com/cookieconsent@3.1.1/build/cookieconsent.min.js defer></script><script src=https://unpkg.com/pangu@4.0.7/dist/browser/pangu.min.js defer></script><script src=https://unpkg.com/cell-watermark@1.0.3/src/watermark.min.js defer></script><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async defer></script><script src=https://unpkg.com/pace-js@1.2.4/pace.min.js async defer></script><script src=/js/flyfish.js defer></script><script>window.config={autoBookmark:!0,code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:50},comment:{enable:!0,expired:!1,giscus:{darkTheme:"dark_dimmed",lightTheme:"light",origin:"https://giscus.app"}},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},enablePWA:!0,lightgallery:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!0,left:"$$",right:"$$"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"},{display:!1,left:"$",right:"$"}],strict:!1},pangu:{enable:!0,selector:"article"},search:{algoliaAppID:"LKM6MKQ6NB",algoliaIndex:"index",algoliaSearchKey:"c9bb91e3a1318f3c9efae98daa87bca5",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},siteTime:"2020-06-10T15:42:01+08:00",version:"v0.3.9-RC",watermark:{colspacing:30,content:'<img style="height: 0.85rem;" src="/logo.png" alt="logo" /> 贺志飞',enable:!0,fontfamily:"LiuJianMaoCao-Regular, 钟齐流江毛草",fontsize:1.1,height:20,opacity:.0125,rotate:15,rowspacing:60,width:150}}</script><script src=/js/theme.min.js defer></script><script src=/js/custom.min.js defer></script></body></html>